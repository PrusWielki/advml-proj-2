{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "1. Find some good candidates for binary classification including ensembles\n",
    "    1. 5 different models\n",
    "2. Write a pipeline for testing and analysis of the results\n",
    "3. Create code documentation :)\n",
    "4. Maybe extract the functions to separate .py to make the notebook cleaner\n",
    "3. Write the report\n",
    "4. Prepare the presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis,\n",
    "    QuadraticDiscriminantAnalysis,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate parameters\n",
    "\n",
    "- takes any number of arrays and returns an array of dictionaries with keys as array names and values, all possible combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get name of a variable as string\n",
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateParameters(arrays):\n",
    "    allCombinations = list(itertools.product(*arrays))\n",
    "    allCombinations = [list(elem) for elem in allCombinations]\n",
    "    arrayNames = []\n",
    "    for array in arrays:\n",
    "        arrayNames.append(namestr(array, globals())[0])\n",
    "    return [dict(zip(arrayNames, value)) for value in allCombinations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'arr1': 1, 'arr2': 3},\n",
       " {'arr1': 1, 'arr2': 4},\n",
       " {'arr1': 1, 'arr2': 8},\n",
       " {'arr1': 2, 'arr2': 3},\n",
       " {'arr1': 2, 'arr2': 4},\n",
       " {'arr1': 2, 'arr2': 8},\n",
       " {'arr1': 7, 'arr2': 3},\n",
       " {'arr1': 7, 'arr2': 4},\n",
       " {'arr1': 7, 'arr2': 8}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1 = [1, 2, 7]\n",
    "arr2 = [3, 4, 8]\n",
    "generateParameters([arr1, arr2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring function, based on which, the best model is selected. Score is calculated according to task description: +10 points for each correctly classified positive class, -200 points for each feature used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScore(y_true, y_pred, featuresUsed):\n",
    "    score = 0\n",
    "    correct = 0\n",
    "    for i, y in enumerate(y_true):\n",
    "        if y == y_pred[i]:\n",
    "            correct += 1\n",
    "    score = 10 * correct - 200 * featuresUsed\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main experiment loop, each chosen classifier is tested with different methods of feature selection and different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performExperiment(X_train, y_train, X_test, y_test, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    finalResult = getScore(y_test, y_pred, len(X_train[0]))\n",
    "    # Take only 1000 of the highest class 1 probabilities\n",
    "    # TODO: Turn it into a dataframe and keep track of the indexes\n",
    "    # finalResult = np.sort(result[:, 1])[::-1][:1000]\n",
    "\n",
    "    return finalResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- definition of available models and feature selection methods\n",
    "\n",
    "- getters for models and feature selection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelType(Enum):\n",
    "    LDA = 0\n",
    "    QDA = 1\n",
    "    DecisionTree = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(modelType, arguments):\n",
    "    match modelType:\n",
    "        case ModelType.LDA:\n",
    "            return LinearDiscriminantAnalysis(**arguments)\n",
    "        case ModelType.QDA:\n",
    "            return QuadraticDiscriminantAnalysis(**arguments)\n",
    "        case ModelType.DecisionTree:\n",
    "            return DecisionTreeClassifier(**arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelectorType(Enum):\n",
    "    NoFeatureSelection = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoFeatureSelection:\n",
    "    def fit(X):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureSelector(selectorType, arguments):\n",
    "    match selectorType:\n",
    "        case FeatureSelectorType.NoFeatureSelection:\n",
    "            return NoFeatureSelection(**arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the training and test data in a format specified by task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Dataset/x_test.txt\") as file:\n",
    "    X_test = [[float(digit) for digit in line.split()] for line in file]\n",
    "\n",
    "\n",
    "with open(\"./Dataset/x_train.txt\") as file:\n",
    "    X_train = [[float(digit) for digit in line.split()] for line in file]\n",
    "\n",
    "\n",
    "with open(\"./Dataset/y_train.txt\") as file:\n",
    "    y_train = [[float(digit) for digit in line.split()] for line in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the X_test there are 500 features and 5000 observations, y_train contains 5000 values, X_train contains 500 features and 5000 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test datapoints: 5000  features: 500\n",
      "X_train datapoints: 5000  features: 500\n",
      "y_train datapoints: 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test datapoints:\", len(X_test), \" features:\", len(X_test[0]))\n",
    "print(\"X_train datapoints:\", len(X_train), \" features:\", len(X_train[0]))\n",
    "print(\"y_train datapoints:\", len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create parameter arrays\n",
    "\n",
    "# LDA\n",
    "solver = [\"svd\"]\n",
    "ldaParameters = generateParameters([solver])\n",
    "\n",
    "\n",
    "models = [{\"model\": ModelType.LDA, \"parameters\": ldaParameters}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureSelectors = [\n",
    "    {\"model\": FeatureSelectorType.NoFeatureSelection, \"parameters\": [{}]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main experiment loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. for now just do one train test split\n",
    "2. For each of the models:\n",
    "    1. For each of the feature selectors:\n",
    "        1. Calculate the score of the given combination of model+feature selector based on the +10 for each correct prediction and -200 for each feature used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conducting the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Patryk\\Repos\\advml-proj-2\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.47 s\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results=[]\n",
    "for model in models:\n",
    "    for parameters in model['parameters']:\n",
    "        X_split_train, X_split_test, y_split_train, y_split_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "        \n",
    "        result = performExperiment(X_train=X_split_train, y_train=y_split_train, X_test=X_split_test, \n",
    "        y_test=y_split_test, model=getModel(model['model'], parameters))\n",
    "\n",
    "        results.append([result,model['model'].name,parameters])\n",
    "with open(\"./Results/results\", \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDf = pd.DataFrame(results, columns=[\"score\", \"model\", \"parameters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>model</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-91740</td>\n",
       "      <td>LDA</td>\n",
       "      <td>{'solver': 'svd'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score model         parameters\n",
       "0 -91740   LDA  {'solver': 'svd'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./Results/results\", \"rb\") as input_file:\n",
    "    results = pickle.load(input_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
